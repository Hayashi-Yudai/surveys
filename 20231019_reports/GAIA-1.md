GAIA-1: A Generative World Model for Autonomous Driving
===

論文：<https://arxiv.org/abs/2309.17080>

2023/09/29

著者：Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, Gianluca Corrado


Wayve というイギリスの自動運転スタートアップの人たち

（まとめ @wakodai）


## どんなもの？
+ GAIA-1 (Generative AI for Autonomy) という生成的世界モデルの紹介
+ ビデオ、テキスト、アクションを入力として、現実的な運転シナリオを生成する
+ 生成した運転シナリオを自動運転車の学習に用いることで自動運転開発を加速する

## 先行研究と比べて何がすごい？
+ ジェネレーティブ・ワールド・モデル￼￼であること:
  + GAIA-1は、ビデオ、テキスト、アクションの入力を活用して現実的な運転シナリオを生成することができる。
  + これにより、自車の行動やシーンの特徴をきめ細かく制御できる。
  + これにより、自動運転技術のトレーニングを強化し、迅速に行うことができる。
+ 文脈認識と一般化:
  + GAIA-1の学習した表現は、文脈を認識して、将来の出来事を予測できる。
  + つまり現実世界のシナリオを理解できるようになる。
  + これにより、より現実的な状況に一般化して、運転モデルの自律性を向上させる能力も備える。
    + （ありえないシーンを生成しても意味がなく、現実に起こりそうなシーンを生成することが自動運転開発にとって重要）
+ Neural Simulator:
  + GAIA-1は、自動運転システムのトレーニングと検証に必要な、敵対的な例を含む無限のデータを生成することができる。
  + これにより、さまざまなシナリオを検討し、システムパフォーマンスを向上させることができる。
## どうやって有効だと検証した？
https://www.youtube.com/watch?v=5Jx2QgEUZUI&t=2s


## 技術や手法の肝は？
+ すべての入力（ビデオ、テキスト、アクション）からの情報を共通の表現にエンコードする
  + ビデオ、テキスト、アクションのそれぞれのエンコード手法について書かれている
+ 画像トークン
  + ビデオの各画像フレームは、離散的なトークンとして表されます。
  + これを実現するために、離散化のための事前学習された画像トークナイザーを使用する（事前学習の詳細については、セクション2.2)
  + T個の画像（x1, . . . , xT）のシーケンスを考え、このシーケンスの各画像xtは、事前学習された画像トークナイザーを使用して n = 576の離散的なトークンに離散化される。
  + これらの離散的なトークンは、ワールドモデルとともに訓練される埋め込みレイヤーを介してd次元の空間にマッピングされます
+ テキストトークン
  + 各タイムステップtで、テキストとアクションの両方からの情報を取り入れます。テキスト入力は、事前学習されたT5-largeモデル[24]を使用してエンコードされ、タイムステップごとにm = 32のテキストトークンが得られる。
  + これらのトークンは、ワールドモデルと連携して訓練される線形レイヤーを通じてd次元の空間にマッピングされる。
  + このプロセスにより、テキスト表現ct = (ct,1, . . . , ct,m) ∈ Rm×dが得られます。 

+ アクショントークン
  + アクションについては、2つのスカラー値（速度と曲率を表す）を考慮する。
  + スカラーは、ワールドモデルとともに訓練される線形レイヤーを介して独立してd次元の空間にマッピングされる。
  + したがって、タイムステップtでのアクションは、at = (at,1, . . . , at,l) ∈ Rl×dとして表されます。

...こうして各入力をトークン化し、ワールドモデルに入力するらしいが、それ以降は読めていない（次回に発表できるといい）
![Alt text](GAIA-1/image2023-10-5_14-2-27.png)

...大規模走行データで学習した結果、車、バス、歩行者、自転車、道路レイアウト、建物、信号機さえも含む、静的要素と動的要素のような重要な概念を理解し、切り離すこともできるらしい。

## 議論はある？

## 次に読むべき論文は？

