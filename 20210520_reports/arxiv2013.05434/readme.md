When is it permissible for artifical intelligence to lie?: A trust-based approach
===

2021/5/10
Tae Wan Kim, Tong(Joy) Lu, Kyusong Lee, Zhaoqi Cheng, Yanhan Tang and John Hooker

https://arxiv.org/abs/2103.05434

（まとめ：井上嵩浩 as @takinou0）

---

## どんなもの？

+ Abstract
	+ 会話AIは人間の振る舞いを真似ており、これには嘘や偽証なども含まれる。
	+ 交渉において、嘘は必要なものである。
	+ 本書では、交渉の場という条件で、どのような嘘が倫理的にOKで、どのような嘘が倫理的にNGかのフレームワークを提言する。
	+ また、AIが嘘をつく場合に考慮すべき要素を概要的に説明する。

+ 私的感想
	+ 実業務だと、どうやって正しい情報をガイドするかに悩むことが多いため、どうやってAI嘘をつかせるかという考えが新しく興味を持った。

---

## どうやって有効だと検証した？

+ Experiments
	+ 理論だけなので実験はない。

---

## 技術や手法の肝は？

+ 以下の考え方を導入する
	+ ものを販売する際に、「このエリアで、この店の価格より安いところはありませんよ」というのは、客に危害が及ばないため倫理的にセーフ
	+ 安全性に関して嘘をつくのは、倫理的にアウト

+ 嘘をつく場合の効果を定義する

```math
¥max_{b} ¥theta E(x|y) + (1 - ¥theta) E(x) - C(b)
```

	- b:嘘の数
	- x:会話の対象になっているものの、実際の値（本当の製品価格など）
	- y:会話の価値
	- E(x|y):yを与えたときの、AIの会話の価値（消費者の期待値）
	- E(x):AIの会話の価値（ユーザの期待値）
	- ¥theta:AIの会話内容に消費者がどの程度左右されるか　0:信用していない ≦ ¥theta ≦ 1:信用している
	- C(b):嘘bをつく事でAI（の持ち主）が支払うコスト　※評判が悪くなることなど


最適な値を取るとき、以下の数式になる。
```math
¥max_{b} ¥theta (E(x) + b - hat{b}) + (1-¥theta) E(x) - C(b)
```

ユーザはだんだんAIのいうことを信用しなくなり、b*=¥hat{b}となる
ユーザはAIを信用しなくなるため、AIの嘘の効果もなくなる。

ここから考えを進めると、
嘘をつくAIは、AIの持ち主に効果をもたらす。
同時に、決して嘘をつかない道徳的なAIは、消費者とAIの持ち主に両方に不利に働く。


+ 文化的要素
国によって、どの程度の嘘までが是とされるかが変わる。
	+ USでは許される。
	+ 北欧スカンジナビアの国々では許されない。
	+ スカンジナビアでは、人間間の信義は重要と考えられており、AIとの対話でも同様と思われる。このため、USで最適化されたAIは、スカンジナビアでは最適とはいえない。
	+ どちらがより倫理的ということもなく、文化の違いである。
	+ 西側諸国は文脈にあまり依存しないコミュニケーションをとっている
	+ 関係性を重んじる文化の場合、長い期間をかけて双方の信頼を作ることが有用である。
	
+ チャットボットの教育方法
	+ 
	


---

## 議論はある？

+ Discussion 節はなし

---

## 先行研究と比べて何がすごい？

+ 不明

---

## 次に読むべき論文は？

+ Masayuki Okamoto, Wizard of oz method for learning dialog agents
　対話システムの教科書や論文で時折Woz法の名前がててくるため、読んでおく。
