When is it permissible for artifical intelligence to lie?: A trust-based approach
===

2021/5/10
Tae Wan Kim, Tong(Joy) Lu, Kyusong Lee, Zhaoqi Cheng, Yanhan Tang and John Hooker

https://arxiv.org/abs/2103.05434

（まとめ：井上嵩浩 as @takinou0）

---

## どんなもの？

+ Abstract
	+ 会話AIは人間の振る舞いを真似ており、これには嘘や偽証なども含まれる。
	+ 交渉において、嘘は必要なものである。
	+ 本書では、交渉の場という条件で、どのような嘘が倫理的にOKで、どのような嘘が倫理的にNGかのフレームワークを提言する。
	+ また、AIが嘘をつく場合に考慮すべき要素を概要的に説明する。

+ 私的感想
	+ 実業務だと、どうやって正しい情報をガイドするかに悩むことが多いため、どうやってAI嘘をつかせるかという考えが新しく興味を持った。

---

## どうやって有効だと検証した？

+ Experiments
	+ 理論だけなので実験はない。

---

## 技術や手法の肝は？

+ 以下の考え方を導入する
	+ ものを販売する際に、「このエリアで、この店の価格より安いところはありませんよ」というのは、客に危害が及ばないため倫理的にセーフ
	+ 安全性に関して嘘をつくのは、倫理的にアウト

+ 嘘をつく場合の効果を定義する

```math
¥max_{b} ¥theta E(x|y) + (1 - ¥theta) E(x) - C(b)
```
+ 変数紹介
	+ b:嘘の数
	+ x:会話の対象になっているものの、実際の値（本当の製品価格など）
	+ y:会話の価値
	+ E(x|y):yを与えたときの、AIの会話の価値（消費者の期待値）
	+ E(x):AIの会話の価値（ユーザの期待値）
	+ ¥theta:AIの会話内容に消費者がどの程度左右されるか　0:信用していない ≦ ¥theta ≦ 1:信用している
	+ C(b):嘘bをつく事でAI（の持ち主）が支払うコスト　※評判が悪くなることなど


最適な値を取るとき、以下の数式になる。
```math
¥max_{b} ¥theta (E(x) + b - hat{b}) + (1-¥theta) E(x) - C(b)
```

ユーザはだんだんAIのいうことを信用しなくなり、b*=¥hat{b}となる
ユーザはAIを信用しなくなるため、AIの嘘の効果もなくなる。

ここから考えを進めると、
嘘をつくAIは、AIの持ち主に効果をもたらす。
同時に、決して嘘をつかない道徳的なAIは、消費者とAIの持ち主に両方に不利に働く。


## 文化的要素
国によって、どの程度の嘘までが是とされるかが変わる。
	+ USでは許される。
	+ 北欧スカンジナビアの国々では許されない。
	+ スカンジナビアでは、価格交渉における人間間の信頼度は重要と考えられており、AIとの対話でも同様となる。一方、USでは信頼度はあまり高くなくても問題ない。
	+ このため、USで最適化されたAIは、スカンジナビアでは最適とはいえない。
	+ 国ごと・文化ごとの、最適モデルを作るには、実証実験が必要である
	+ 交渉の意味は、国を問わず、双方の折り合いを付けれるポイントを探すことであるが、そのためのマナーは国によって異なる。
	+ どちらがより倫理的ということもなく、文化の違いである。
	+ 西側諸国は文脈にあまり依存しないコミュニケーションをとっている
	+ 関係性を重んじる文化の場合、長い期間をかけて双方の信頼を作ることが有用である。
	
## チャットボットの教育方法
+ データの集め方
	+ AIの教育にはデータが重要だが、チャットボット用のいいデータを揃えるにはお金がかかる。
	+ このため、大概の商業チャットボットは、特定の領域に絞ってルールベースで動いている。
	+ 機械学習のアプローチを採用する場合、倫理的な嘘をつけるチャットボットを作るには、実際の交渉での会話データが必要である。
	+ 非倫理的な対話が教え込まれないようにすることが大切である。
	+ 信じるように差し向けているかどうかを検知できることが大切であり、信頼度が閾値より少なければ、非倫理的な交渉にならないように、嘘をつかないモードに変更しないといけない。
	
+ モデルのトレーニング
	+ pre training
		+ 一般的な会話データを大量に読ませることで、word、sentence、会話の流れなどをモデルに学習させる
		+ 本当は倫理的な会話データだけを読ませたいが、データ準備が難しいため、一般的なデータを使って事前学習させる 
	+ domein fine tuning
		+ 小さなデータセットだと過学習してしまうため、さまざまなfine tuningのテクニックを使う 
	+ 強化学習
		+ 倫理的な嘘のデータはスパースな出来事なので、モデル作成後の強化学習が役に立つ

---

## 議論はある？

+ Discussion 節はなし

---

## 先行研究と比べて何がすごい？

+ 倫理モデルを作ったことが偉い、らしい。

---

## 次に読むべき論文は？

+ Masayuki Okamoto, Wizard of oz method for learning dialog agents
　対話システムの教科書や論文で時折Woz法の名前がててくるため、読んでおく。
