Deep Residual Learning for Image Recognition
===

論文：<https://arxiv.org/abs/1512.03385>

2015/12/10

著者：Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun

Microsoft Research の人たち

（まとめ @wakodai）

---

## どんなもの？

+ 深層学習ネットワークの訓練を容易にするための残差学習フレームワークの紹介
+ (当時の) ImageNet データセットでの先行結果はすべて「非常に深い」モデルを利用し、その深さは 16 から 30 に及んだ。
+ 層を深くすることの問題
  + 勾配消失/爆発問題 : Xavier, He の初期値や、Batch Normalization によって大幅に解決され、バックプロパゲーションを用いた確率的勾配降下法で*数十層の*ネットワークが収束を開始することが可能
  + 劣化問題 : ネットワークの深さが増すにつれて、精度が飽和し、その後急速に劣化する。この劣化は過学習によるものではない事が報告されている
  ![](./ResNet/Pasted%20image%2020230614111905.png)  
+ 理論的には深いモデルは浅いモデルよりも学習誤差が大きくならないはず (表現力が増すはずなので)
+ だが、現在手元にあるソルバーでは、同等以上の解を見つけることができない（あるいは**実現可能な時間内に見つけることができない**）ことがわかった
+ 本論文では、深層残差学習フレームワークを導入することで、この劣化問題に対処する

---

## どうやって有効だと検証した？

+ ImageNetデータセットを使用して、最大 152 層までの深さの残差ネットワークを評価した
  + これは、(当時の SOTA の) VGG ネットよりも 8 倍深い
+ ImageNet テストセットで top-5 err. 3.57% を達成。これは、ILSVRC 2015 分類タスクで 1 位を獲得。

  ![](./ResNet/ILSVRC_results.jpg)

+ 同じ深さでスキップ接続しない版と比べた

  ![](./ResNet/Pasted%20image%2020230614201827.png)

  ![](./ResNet/Pasted%20image%2020230615111017.png)

+ CIFAR-10, PASCAL, COCO もやった (良かった)


---

## 技術や手法の肝は？

+ スキップ接続！

  ![](./ResNet/Pasted%20image%2020230614201533.png)

+ 劣化問題（図1左）に関する直感に反する現象が動機である

+ 層を追加していった際に、仮にそれが恒等写像 (identity mapping) として学習されれば、層を深くしても学習誤差が大きくならないはず
  + identity mapping: 入力がそのまま出力になるような関数
  + -> identity mapping 層なら、増やしてもなにも変わらないはず -> 学習誤差大きくならないはず (良くもならないが)

+ しかし実際は劣化が起こっている
  + -> NN は (そのままでは) identity mapping を近似することが困難である可能性を示唆している
+ そこで、**積層が残差関数 F(x) := H(x) - x を近似するように明示的にしてあげる**ことにした (＝スキップ接続)
+ 残差学習では、identity mapping が最適であれば、ソルバーが重みをゼロに近づけるだけで、identity mapping に近づけることができる。
+ 実際のケースでは、identity mapping が最適である可能性は低いが、我々の再定義は問題の前提条件を整えるのに役立つかもしれない。
+ 最適な関数がゼロマッピングよりも ideneity マッピングに近い場合、ソルバーにとって、関数を新たに学習するよりも、identity マッピングを参照して摂動を見つける方が簡単であるはずである。我々は実験（図7）により、学習された残差関数は一般に小さな応答を持つことを示し、同一性写像が妥当な事前調整を提供することを示唆した

  ![](./ResNet/Pasted%20image%2020230615120627.png)


---

## 議論はある？

+ CIDAR-10 で 1202 層まで深くしたら 110 層のよりテスト結果が悪くなった
+ 過学習が起きていると考える
+ このデータセットに対しては不必要に大きいのかもしれない
+ このデータセットで最良の他のモデルは maxout/dropout などの強い正則化を用いている
+ 我々は最適化の難しさに焦点をあてたかったのでこれらは採用していない
+ より協力な正則化と組み合わせる事で結果が改善される可能性があり、今後研究していく予定

---

## 先行研究と比べて何がすごい？

+ 当時困難とされていたより深いネットワークの訓練を可能にした

---

## 次に読むべき論文は？

+ EfficientNet ...かなぁ？（ImageNet の成績をさらによくしたとのことなので）
  + https://arxiv.org/abs/1905.11946


## (番外編)
+ 論文では、残差学習により勾配消失問題が改善するという話はなかったが、一般的? にはそういう効果があるというのをよく見るので、ちょっと調べてみた

  ![](./ResNet/Pasted%20image%2020230615122619.png)
